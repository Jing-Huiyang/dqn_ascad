import os
import numpy as np
import torch
import torch.nn.functional as F
from torchvision.transforms import transforms
from src.dataloader import ToTensor_trace, Custom_Dataset
from src.net import MLP, CNN
from src.utils import perform_attacks, NTGE_fn
from tqdm import tqdm

def generate_predictions_from_existing_models():
    """
    ä»ç°æœ‰çš„è®­ç»ƒæ¨¡å‹ä¸­ç”Ÿæˆé¢„æµ‹æ–‡ä»¶
    """
    # ==================== å‚æ•°è®¾ç½® ====================
    dataset = "ASCAD_variable_desync50"
    model_type = "mlp"
    leakage = "HW"
    byte = 2
    nb_traces_attacks = 2000
    nb_attacks = 50
    
    # ==================== è·¯å¾„è®¾ç½® ====================
    root = "./Result/"
    save_root = root + dataset + "_" + model_type + "_byte" + str(byte) + "_" + leakage + "/"
    model_root = save_root + "models/"
    
    print(f"Looking for models in: {model_root}")
    
    # ==================== è®¾å¤‡è®¾ç½® ====================
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # ==================== æ•°æ®åŠ è½½ ====================
    print("Loading datasets...")
    try:
        dset_test = Custom_Dataset(dataset=dataset, leakage=leakage,
                               transform=transforms.Compose([ToTensor_trace()]),
                               byte=byte, val_ratio=0.1)
        
        # è·å–æ”»å‡»æ•°æ® - åœ¨choose_phaseä¹‹å‰ä¿å­˜åŸå§‹æ•°æ®
        correct_key = dset_test.correct_key
        X_attack = dset_test.X_attack
        plt_attack = dset_test.plt_attack
        
        # ç¡®ä¿X_attackæ˜¯numpyæ•°ç»„
        if not isinstance(X_attack, np.ndarray):
            raise ValueError(f"X_attack should be numpy array, got {type(X_attack)}")
        
        num_sample_pts = X_attack.shape[-1]
        
        print(f"Attack data shape: {X_attack.shape}")
        print(f"Correct key: {correct_key}")
        
    except Exception as e:
        print(f"Error loading datasets: {str(e)}")
        return
    
    # ==================== è®¾ç½®ç±»åˆ«æ•° ====================
    if leakage == 'HW':
        classes = 9
    elif leakage == 'ID':
        classes = 256
    else:
        print(f"Invalid leakage model: {leakage}")
        return
    
    # ==================== æŸ¥æ‰¾ç°æœ‰æ¨¡å‹ ====================
    model_files = []
    config_files = []
    
    for i in range(100):  # å‡è®¾æœ€å¤š100ä¸ªæ¨¡å‹
        model_path = model_root + f"model_{i}_byte{byte}.pth"
        config_path = model_root + f"model_configuration_{i}.npy"
        
        if os.path.exists(model_path) and os.path.exists(config_path):
            model_files.append((i, model_path))
            config_files.append((i, config_path))
    
    print(f"Found {len(model_files)} trained models")
    
    if len(model_files) == 0:
        print("No trained models found! Please train models first.")
        return
    
    # ==================== ç”Ÿæˆé¢„æµ‹ ====================
    ensemble_predictions = []
    all_ind_GE = []
    
    print("Generating predictions from existing models...")
    
    for model_idx, model_path in tqdm(model_files, desc="Processing models"):
        try:
            # åŠ è½½æ¨¡å‹é…ç½®
            config_path = model_root + f"model_configuration_{model_idx}.npy"
            config = np.load(config_path, allow_pickle=True).item()
            
            # åˆ›å»ºæ¨¡å‹
            if model_type == "mlp":
                model = MLP(config, num_sample_pts, classes).to(device)
            elif model_type == "cnn":
                model = CNN(config, num_sample_pts, classes).to(device)
            else:
                print(f"Invalid model type: {model_type}")
                continue
            
            # åŠ è½½æ¨¡å‹æƒé‡
            model.load_state_dict(torch.load(model_path, map_location=device))
            model.eval()
            
            # ç”Ÿæˆé¢„æµ‹
            with torch.no_grad():
                attack_traces = torch.from_numpy(X_attack[:nb_traces_attacks]).to(device).unsqueeze(1).float()
                predictions_wo_softmax = model(attack_traces)
                predictions = F.softmax(predictions_wo_softmax, dim=1)
                predictions = predictions.cpu().numpy()
            
            ensemble_predictions.append(predictions)
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ä¸ªä½“ç»“æœ
            result_path = model_root + f"result_{model_idx}_byte{byte}.npy"
            
            if os.path.exists(result_path):
                # åŠ è½½ç°æœ‰ç»“æœ
                individual_result = np.load(result_path, allow_pickle=True).item()
                individual_GE = individual_result["GE"]
                print(f"Model {model_idx}: Loaded existing GE = {individual_GE[-1]}")
            else:
                # è®¡ç®—æ–°çš„ç»“æœ
                print(f"Model {model_idx}: Computing new GE...")
                individual_GE, _ = perform_attacks(
                    nb_traces_attacks, predictions, plt_attack, correct_key,
                    dataset=dataset, nb_attacks=nb_attacks, shuffle=True,
                    leakage=leakage, byte=byte
                )
                individual_NTGE = NTGE_fn(individual_GE)
                
                # ä¿å­˜ç»“æœ - ä½¿ç”¨allow_pickle=Trueæ¥ä¿å­˜å­—å…¸
                result_dict = {"GE": individual_GE, "NTGE": individual_NTGE}
                np.save(result_path, result_dict, allow_pickle=True)  # type: ignore
                print(f"Model {model_idx}: Computed GE = {individual_GE[-1]}")
            
            all_ind_GE.append(individual_GE[-1])
            
        except Exception as e:
            print(f"Error processing model {model_idx}: {str(e)}")
            continue
    
    # ==================== ä¿å­˜é›†æˆé¢„æµ‹æ–‡ä»¶ ====================
    if len(ensemble_predictions) > 0:
        print(f"\nSaving predictions for {len(ensemble_predictions)} models...")
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        ensemble_predictions = np.array(ensemble_predictions)
        all_ind_GE = np.array(all_ind_GE)
        
        # ä¿å­˜æ–‡ä»¶
        np.save(model_root + "all_ensemble_predictions.npy", ensemble_predictions)
        np.save(model_root + "all_ind_GE.npy", all_ind_GE)
        
        print("âœ… Successfully saved:")
        print(f"   - all_ensemble_predictions.npy: shape {ensemble_predictions.shape}")
        print(f"   - all_ind_GE.npy: shape {all_ind_GE.shape}")
        print(f"   - GE values range: {all_ind_GE.min():.4f} to {all_ind_GE.max():.4f}")
        
        # æ‰“å°ä¸€äº›ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š Model Performance Statistics:")
        print(f"   - Best model GE: {all_ind_GE.min():.4f}")
        print(f"   - Worst model GE: {all_ind_GE.max():.4f}")
        print(f"   - Average GE: {all_ind_GE.mean():.4f}")
        print(f"   - Standard deviation: {all_ind_GE.std():.4f}")
        
        return True
    else:
        print("âŒ No valid predictions generated!")
        return False

if __name__ == "__main__":
    print("ğŸš€ Generating predictions from existing models...")
    success = generate_predictions_from_existing_models()
    
    if success:
        print("\nâœ… Prediction generation completed successfully!")
        print("Now you can run the model selection methods:")
        print("   python select_models.py")
        print("   python compare_selection_methods.py")
    else:
        print("\nâŒ Failed to generate predictions!") 