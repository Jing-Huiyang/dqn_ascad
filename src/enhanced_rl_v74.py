import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import random
import math
from collections import namedtuple, deque
import matplotlib.pyplot as plt
import seaborn as sns
import json
import pickle
from datetime import datetime
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from typing import List, Tuple, Dict, Optional
import logging
import time
import os
from src.enhanced_utils_parallel import perform_attacks_ensemble_parallel_enhanced, adaptive_process_count
from src.utils import NTGE_fn

# å¯¼å…¥Qè¡¨å¯è§†åŒ–å™¨
try:
    from src.q_table_visualizer import QTableVisualizer
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„ç±»é¿å…é”™è¯¯
    class QTableVisualizer:
        def __init__(self, *args, **kwargs):
            pass
        def record_q_values(self, *args, **kwargs):
            pass
        def generate_comprehensive_report(self):
            pass

# Enhanced Experience Replay for Global Strategy Learning
Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))

class PrioritizedReplayMemory:
    """ä¼˜å…ˆç»éªŒå›æ”¾ï¼Œæé«˜å­¦ä¹ æ•ˆç‡"""
    def __init__(self, capacity, alpha=0.6, beta=0.4):
        self.capacity = capacity
        self.alpha = alpha
        self.beta = beta
        self.memory = []
        self.priorities = np.zeros(capacity, dtype=np.float32)
        self.position = 0
        
    def push(self, *args):
        max_priority = self.priorities.max() if self.memory else 1.0
        
        if len(self.memory) < self.capacity:
            self.memory.append(None)
        
        self.memory[self.position] = Transition(*args)
        self.priorities[self.position] = max_priority
        self.position = (self.position + 1) % self.capacity
        
    def sample(self, batch_size):
        if len(self.memory) == self.capacity:
            priorities = self.priorities
        else:
            priorities = self.priorities[:self.position]
            
        probabilities = priorities ** self.alpha
        probabilities /= probabilities.sum()
        
        indices = np.random.choice(len(self.memory), batch_size, p=probabilities)
        samples = [self.memory[idx] for idx in indices]
        
        # Importance-sampling weights
        total = len(self.memory)
        weights = (total * probabilities[indices]) ** (-self.beta)
        weights /= weights.max()
        
        return samples, indices, torch.FloatTensor(weights)
        
    def update_priorities(self, indices, priorities):
        for idx, priority in zip(indices, priorities):
            self.priorities[idx] = priority
            
    def __len__(self):
        return len(self.memory)

class GlobalStrategyDQN(nn.Module):
    """
    å…¨å±€ç­–ç•¥æ„ŸçŸ¥çš„DQNç½‘ç»œ
    - ç›´æ¥è¾“å‡ºæ¯ä¸ªæ¨¡å‹çš„é€‰æ‹©æ¦‚ç‡/ä»·å€¼
    - å…·å¤‡å…¨å±€ç»„åˆæ„ŸçŸ¥èƒ½åŠ›
    """
    def __init__(self, total_models, hidden_dim=512):
        super(GlobalStrategyDQN, self).__init__()
        self.total_models = total_models
        
        # Multi-head attention for global awareness
        self.self_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim, num_heads=8, dropout=0.1
        )
        
        # Feature extraction layers
        self.input_projection = nn.Linear(total_models, hidden_dim)
        self.feature_layers = nn.ModuleList([
            nn.Linear(hidden_dim, hidden_dim),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Linear(hidden_dim, hidden_dim)
        ])
        
        # Global context processing
        self.global_context = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU()
        )
        
        # Output layers for each model
        self.model_values = nn.Linear(hidden_dim + hidden_dim // 4, total_models)
        self.combination_value = nn.Linear(hidden_dim + hidden_dim // 4, 1)
        
        # Dropout for regularization
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, state):
        # Input projection
        x = F.relu(self.input_projection(state))
        x = self.dropout(x)
        
        # Feature extraction with residual connections
        for layer in self.feature_layers:
            residual = x
            x = F.relu(layer(x))
            x = self.dropout(x)
            x = x + residual  # Residual connection
        
        # Self-attention for global awareness
        if len(x.shape) == 2:
            x_unsqueezed = x.unsqueeze(0)  # Add sequence dimension
        else:
            x_unsqueezed = x
            
        attn_output, _ = self.self_attention(x_unsqueezed, x_unsqueezed, x_unsqueezed)
        x = attn_output.squeeze(0) if len(x.shape) == 2 else attn_output
        
        # Global context
        if len(x.shape) == 2:
            # x is [batch_size, hidden_dim]
            global_features = self.global_context(x.mean(dim=0, keepdim=True))  # [1, hidden_dim//4]
            global_features = global_features.expand(x.shape[0], -1)            # [batch_size, hidden_dim//4]
        else:
            # x is [hidden_dim]
            global_features = self.global_context(x)                             # [hidden_dim//4]
            global_features = global_features.unsqueeze(0)                       # [1, hidden_dim//4]
        
        # Combine local and global features
        combined_features = torch.cat([x, global_features], dim=-1)
        
        # Output model values and combination value
        model_values = self.model_values(combined_features)
        combination_value = self.combination_value(combined_features)
        
        return model_values, combination_value

class EnhancedModelSelector:
    """
    å¢å¼ºçš„æ¨¡å‹é€‰æ‹©å™¨ï¼š
    1. å…¨å±€ç­–ç•¥æ„ŸçŸ¥
    2. äºŒè¿›åˆ¶å‘é‡è¡¨ç¤º
    3. å®Œæ•´çš„å¯è§£é‡Šæ€§åˆ†æ
    4. é«˜æ•ˆçš„å¹¶è¡Œå¤„ç†
    """
    
    def __init__(self, device, total_models=100, max_models=15):
        self.device = device
        self.total_models = total_models
        self.max_models = max_models
        
        # Enhanced DQN parameters
        self.BATCH_SIZE = 64
        self.GAMMA = 0.99
        self.EPS_START = 0.9
        self.EPS_END = 0.01
        self.EPS_DECAY = 2000
        self.TAU = 0.001
        self.LR = 5e-4
        
        # Initialize networks
        self.policy_net = GlobalStrategyDQN(total_models).to(device)
        self.target_net = GlobalStrategyDQN(total_models).to(device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        
        # Enhanced optimizer with scheduler
        self.optimizer = optim.AdamW(
            self.policy_net.parameters(), 
            lr=self.LR, 
            weight_decay=1e-5
        )
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer, step_size=500, gamma=0.95
        )
        
        # Prioritized experience replay
        self.memory = PrioritizedReplayMemory(20000)
        self.steps_done = 0
        
        # Training metrics for analysis
        self.training_metrics = {
            'rewards': [],
            'losses': [],
            'epsilon_values': [],
            'q_values': [],
            'model_selection_frequency': np.zeros(total_models),
            'episode_lengths': [],
            'final_ge_values': []
        }
        
        # Initialize Q-table visualizer
        self.q_visualizer = None
        self.enable_q_visualization = True
        
    def get_binary_state(self, selected_models: List[int]) -> torch.Tensor:
        """
        è·å–äºŒè¿›åˆ¶çŠ¶æ€è¡¨ç¤º
        Args:
            selected_models: å·²é€‰æ‹©çš„æ¨¡å‹ç´¢å¼•åˆ—è¡¨
        Returns:
            äºŒè¿›åˆ¶çŠ¶æ€å‘é‡ (1è¡¨ç¤ºé€‰æ‹©ï¼Œ0è¡¨ç¤ºæœªé€‰æ‹©)
        """
        state = torch.zeros(self.total_models, device=self.device)
        if selected_models:
            state[selected_models] = 1.0
        return state
        
    def select_action_epsilon_greedy(self, state: torch.Tensor, step: int, 
                                   available_models: List[int], 
                                   selected_models: List[int],
                                   is_training: bool = True) -> int:
        """
        ä½¿ç”¨Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©ä¸‹ä¸€ä¸ªè¦æ·»åŠ çš„æ¨¡å‹
        """
        # Calculate epsilon
        if is_training:
            eps = self.EPS_END + (self.EPS_START - self.EPS_END) * \
                  math.exp(-1. * self.steps_done / self.EPS_DECAY)
            self.steps_done += 1
            self.training_metrics['epsilon_values'].append(eps)
        else:
            eps = 0.0
            
        # Get model values from network
        with torch.no_grad():
            model_values, combination_value = self.policy_net(state.unsqueeze(0))
            model_values = model_values.squeeze()
            
            # Record Q-values for analysis
            if is_training:
                self.training_metrics['q_values'].append(model_values.cpu().numpy())
        
        # Epsilon-greedy selection among available models
        if random.random() < eps and is_training:
            # Exploration: random selection from available models
            action = random.choice(available_models)
        else:
            # Exploitation: select model with highest value among available
            available_values = model_values[available_models]
            best_idx = available_values.argmax().item()
            action = available_models[best_idx]
        
        # Record Q-values for visualization if enabled
        if self.enable_q_visualization and self.q_visualizer is not None and is_training:
            # è¿™é‡Œå…ˆè®°å½•ä¸€ä¸ªå ä½å¥–åŠ±ï¼Œåç»­ä¼šæ›´æ–°
            self.q_visualizer.record_q_values(
                episode=step // 50,  # ç²—ç•¥ä¼°è®¡episode
                state=state,
                q_values=model_values,
                selected_action=action,
                reward=0.0  # å ä½å€¼ï¼Œåç»­æ›´æ–°
            )
            
        return action
        
    def calculate_reward(self, prev_ge: float, new_ge: float, 
                        prev_models: List[int], new_models: List[int],
                        episode_step: int, max_steps: int = 50,
                        prev_ntge: float = None, new_ntge: float = None) -> float:
        """
        è®¡ç®—å¥–åŠ±å‡½æ•°ï¼ˆä»¥NTGEä¸ºæ ¸å¿ƒå†³ç­–æŒ‡æ ‡ï¼‰
        
        Args:
            prev_ge: å‰ä¸€ä¸ªçŠ¶æ€çš„GEå€¼
            new_ge: æ–°çŠ¶æ€çš„GEå€¼
            prev_models: å‰ä¸€ä¸ªçŠ¶æ€çš„æ¨¡å‹åˆ—è¡¨
            new_models: æ–°çŠ¶æ€çš„æ¨¡å‹åˆ—è¡¨
            episode_step: å½“å‰æ­¥æ•°
            max_steps: æœ€å¤§æ­¥æ•°
            prev_ntge: å‰ä¸€ä¸ªçŠ¶æ€çš„NTGEå€¼
            new_ntge: æ–°çŠ¶æ€çš„NTGEå€¼
        """
        # æ ¸å¿ƒç›®æ ‡ï¼šæœ€å°åŒ–NTGEï¼ˆè¾¾åˆ°GE=0çš„æœ€å°‘è½¨è¿¹æ•°ï¼‰
        if new_ntge is not None and new_ntge < float('inf'):
            # æˆåŠŸè¾¾åˆ°GE=0ï¼ŒNTGEè¶Šå°å¥–åŠ±è¶Šé«˜
            success_reward = 1000.0  # ğŸ”§ åŸºç¡€æˆåŠŸå¥–åŠ±ï¼ˆé™ä½é¿å…è¿‡åº¦ä¼˜åŒ–ï¼‰
            
            # NTGEæ•ˆç‡å¥–åŠ±ï¼šæ›´ç²¾ç»†çš„åˆ†çº§ï¼Œçªå‡ºæœ€ä¼˜æ¨¡å‹
            if new_ntge <= 1500:
                ntge_efficiency = 800.0  # æä¼˜ç§€
            elif new_ntge <= 1700:
                ntge_efficiency = 600.0  # å¾ˆä¼˜ç§€  
            elif new_ntge <= 1900:
                ntge_efficiency = 400.0  # ä¼˜ç§€
            elif new_ntge <= 2000:
                ntge_efficiency = 200.0  # è‰¯å¥½
            elif new_ntge <= 5000:
                ntge_efficiency = 100.0  # ä¸€èˆ¬
            else:
                ntge_efficiency = 50.0   # è¾ƒå·®
                
            # NTGEæ”¹è¿›å¥–åŠ±ï¼ˆåªæœ‰åœ¨æœ‰æ¯”è¾ƒå¯¹è±¡æ—¶æ‰è®¡ç®—ï¼‰
            if prev_ntge is not None and prev_ntge != float('inf'):
                ntge_improvement = max(0, (prev_ntge - new_ntge) * 0.1)
            else:
                ntge_improvement = 0
            
            # æ¨¡å‹æ•ˆç‡å¥–åŠ±ï¼šä½¿ç”¨æ›´å°‘çš„æ¨¡å‹è·å¾—æ›´é«˜å¥–åŠ±
            efficiency_bonus = max(0, (self.max_models - len(new_models)) * 50.0)
            
            # é€Ÿåº¦å¥–åŠ±ï¼šæ›´å¿«æ‰¾åˆ°è§£å†³æ–¹æ¡ˆè·å¾—æ›´é«˜å¥–åŠ±
            speed_bonus = max(0, (max_steps - episode_step) * 10.0)
            
            total_reward = success_reward + ntge_efficiency + ntge_improvement + efficiency_bonus + speed_bonus
            return total_reward
        
        # å¦‚æœNTGEä¸ºæ— ç©·å¤§ï¼ˆæ— æ³•è¾¾åˆ°GE=0ï¼‰ï¼ŒåŸºäºGEæ”¹è¿›è®¡ç®—å¥–åŠ±
        elif new_ntge == float('inf'):
            # ğŸ”§ æ— æ³•è¾¾åˆ°GE=0æ—¶ï¼ŒåŸºäºGEç»™äºˆåˆç†å¥–åŠ±
            
            # GEæ”¹è¿›å¥–åŠ±ï¼ˆä¸»è¦æŒ‡æ ‡ï¼‰
            ge_improvement = prev_ge - new_ge
            ge_reward = ge_improvement * 10.0  # é™ä½æƒé‡é¿å…è¿‡åº¦ä¼˜åŒ–
            
            # åŸºäºGEç»å¯¹å€¼çš„å¥–åŠ±ï¼ˆé¿å…æ‰€æœ‰å¥–åŠ±éƒ½æ˜¯è´Ÿæ•°ï¼‰
            if new_ge < 1.0:
                base_reward = 200.0  # å¾ˆå¥½
            elif new_ge < 5.0:
                base_reward = 100.0  # è‰¯å¥½
            elif new_ge < 10.0:
                base_reward = 50.0   # ä¸€èˆ¬
            elif new_ge < 50.0:
                base_reward = 10.0   # è¾ƒå·®
            else:
                base_reward = -10.0  # å¾ˆå·®
            
            # æ¨¡å‹æ•ˆç‡å¥–åŠ±ï¼šä½¿ç”¨æ›´å°‘çš„æ¨¡å‹è·å¾—æ›´é«˜å¥–åŠ±
            efficiency_bonus = max(0, (self.max_models - len(new_models)) * 5.0)
            
            # æƒ©ç½šï¼šæ²¡æœ‰æ”¹å–„
            if new_ge >= prev_ge:
                stagnation_penalty = -20.0
            else:
                stagnation_penalty = 0
            
            # ç»¼åˆå¥–åŠ±
            reward = base_reward + ge_reward + efficiency_bonus + stagnation_penalty
            
        else:
            # å¦‚æœæ²¡æœ‰NTGEä¿¡æ¯ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„GE-basedå¥–åŠ±
            ge_improvement = prev_ge - new_ge
            
            # æ¥è¿‘ç›®æ ‡çš„å¥–åŠ±
            if new_ge < 10:
                proximity_reward = (10 - new_ge) * 10.0
            elif new_ge < 50:
                proximity_reward = (50 - new_ge) * 1.0
            else:
                proximity_reward = 0
            
            # æƒ©ç½š
            model_penalty = len(new_models) * 2.0
            stagnation_penalty = -10.0 if new_ge >= prev_ge else 0
            
            reward = ge_improvement * 20.0 + proximity_reward - model_penalty + stagnation_penalty
        
        return reward

    def evaluate_combination_quality(self, ge: float, ntge: float, model_count: int) -> float:
        """
        è¯„ä¼°æ¨¡å‹ç»„åˆè´¨é‡ï¼ˆä»¥NTGEä¸ºæ ¸å¿ƒæŒ‡æ ‡ï¼‰
        
        Args:
            ge: çŒœæµ‹ç†µå€¼
            ntge: è¾¾åˆ°GE=0æ‰€éœ€è½¨è¿¹æ•°
            model_count: ä½¿ç”¨çš„æ¨¡å‹æ•°é‡
            
        Returns:
            ç»¼åˆè´¨é‡è¯„åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        """
        # NTGEåˆ†æ•°ï¼šNTGEè¶Šå°è¶Šå¥½ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼‰
        if ntge != float('inf'):
            ntge_score = max(0, 100 - ntge * 0.02)  # NTGE=0å¾—100åˆ†ï¼ŒNTGE=5000å¾—0åˆ†
        else:
            # å¦‚æœNTGEä¸ºæ— ç©·å¤§ï¼ŒåŸºäºGEè®¡ç®—åˆ†æ•°
            ntge_score = max(0, 50 - ge * 5)  # GE=0å¾—50åˆ†ï¼ŒGE=10å¾—0åˆ†
        
        # æ¨¡å‹æ•ˆç‡åˆ†æ•°ï¼šä½¿ç”¨æ›´å°‘æ¨¡å‹æ›´å¥½
        efficiency_score = max(0, 30 - model_count * 2)  # 0ä¸ªæ¨¡å‹å¾—30åˆ†ï¼Œ15ä¸ªæ¨¡å‹å¾—0åˆ†
        
        # ç»¼åˆè¯„åˆ†ï¼ˆNTGEæƒé‡æœ€é«˜ï¼‰
        total_score = ntge_score * 0.8 + efficiency_score * 0.2
        
        return total_score

    def should_update_best_result(self, current_best_ge: float, current_best_ntge: float,
                                new_ge: float, new_ntge: float, 
                                current_best_score: float = None) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥æ›´æ–°æœ€ä½³ç»“æœï¼ˆä»¥NTGEä¸ºæ ¸å¿ƒï¼‰
        
        Args:
            current_best_ge: å½“å‰æœ€ä½³GE
            current_best_ntge: å½“å‰æœ€ä½³NTGE
            new_ge: æ–°çš„GE
            new_ntge: æ–°çš„NTGE
            current_best_score: å½“å‰æœ€ä½³ç»¼åˆè¯„åˆ†
            
        Returns:
            æ˜¯å¦åº”è¯¥æ›´æ–°
        """
        # è®¡ç®—æ–°çš„ç»¼åˆè¯„åˆ†
        new_score = self.evaluate_combination_quality(new_ge, new_ntge, 0)  # model_countæš‚æ—¶è®¾ä¸º0
        
        # å¦‚æœå½“å‰æœ€ä½³è¯„åˆ†æœªæä¾›ï¼Œä½¿ç”¨ç®€å•çš„æ¯”è¾ƒé€»è¾‘
        if current_best_score is None:
            # ä¼˜å…ˆè€ƒè™‘NTGE
            if new_ntge < float('inf') and current_best_ntge == float('inf'):
                return True  # ä»æ— æ³•è¾¾åˆ°GE=0åˆ°å¯ä»¥è¾¾åˆ°
            elif new_ntge < float('inf') and current_best_ntge < float('inf'):
                return new_ntge < current_best_ntge  # éƒ½è¾¾åˆ°GE=0ï¼Œé€‰æ‹©NTGEæ›´å°çš„
            elif new_ntge == float('inf') and current_best_ntge == float('inf'):
                # éƒ½æ— æ³•è¾¾åˆ°GE=0ï¼Œæ¯”è¾ƒGE
                return new_ge < current_best_ge
            else:
                return False  # å½“å‰å¯ä»¥è¾¾åˆ°GE=0ï¼Œæ–°çš„æ— æ³•è¾¾åˆ°
        else:
            # ä½¿ç”¨ç»¼åˆè¯„åˆ†æ¯”è¾ƒ
            return new_score > current_best_score
        
    def optimize_model(self):
        """ä¼˜åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨ä¼˜å…ˆç»éªŒå›æ”¾ï¼‰"""
        if len(self.memory) < self.BATCH_SIZE:
            return
            
        # Sample from prioritized replay
        transitions, indices, weights = self.memory.sample(self.BATCH_SIZE)
        batch = Transition(*zip(*transitions))
        
        # Prepare batch data
        state_batch = torch.stack(batch.state).to(self.device)
        action_batch = torch.LongTensor(batch.action).to(self.device)
        reward_batch = torch.FloatTensor(batch.reward).to(self.device)
        done_batch = torch.BoolTensor(batch.done).to(self.device)
        
        # Handle next states (some might be None for terminal states)
        non_final_mask = torch.tensor([s is not None for s in batch.next_state], 
                                    device=self.device, dtype=torch.bool)
        non_final_next_states = torch.stack([s for s in batch.next_state if s is not None]).to(self.device)
        
        # Current Q-values
        model_values, combination_values = self.policy_net(state_batch)
        current_q_values = model_values.gather(1, action_batch.unsqueeze(1)).squeeze()
        
        # Next Q-values
        next_q_values = torch.zeros(self.BATCH_SIZE, device=self.device)
        with torch.no_grad():
            if non_final_mask.any():
                next_model_values, _ = self.target_net(non_final_next_states)
                next_q_values[non_final_mask] = next_model_values.max(1)[0]
        
        # Expected Q-values
        expected_q_values = reward_batch + (self.GAMMA * next_q_values * ~done_batch)
        
        # Compute loss with importance sampling weights
        td_errors = current_q_values - expected_q_values.detach()
        loss = (weights.to(self.device) * (td_errors ** 2)).mean()
        
        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
        self.optimizer.step()
        self.scheduler.step()
        
        # Update priorities
        priorities = (abs(td_errors) + 1e-6).detach().cpu().numpy()
        self.memory.update_priorities(indices, priorities)
        
        # Record training metrics
        self.training_metrics['losses'].append(loss.item())
        
    def update_target_network(self):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        target_net_state_dict = self.target_net.state_dict()
        policy_net_state_dict = self.policy_net.state_dict()
        
        for key in policy_net_state_dict:
            target_net_state_dict[key] = \
                policy_net_state_dict[key] * self.TAU + \
                target_net_state_dict[key] * (1 - self.TAU)
                
        self.target_net.load_state_dict(target_net_state_dict)
        
    def save_analysis_data(self, save_path: str, episode: int):
        """ä¿å­˜å®Œæ•´çš„åˆ†ææ•°æ®"""
        analysis_data = {
            'episode': episode,
            'training_metrics': self.training_metrics,
            'network_state': {
                'policy_net': self.policy_net.state_dict(),
                'target_net': self.target_net.state_dict(),
                'optimizer': self.optimizer.state_dict()
            },
            'hyperparameters': {
                'total_models': self.total_models,
                'max_models': self.max_models,
                'batch_size': self.BATCH_SIZE,
                'gamma': self.GAMMA,
                'learning_rate': self.LR,
                'tau': self.TAU
            },
            'timestamp': datetime.now().isoformat()
        }
        
        # Save as pickle for complete data
        with open(f"{save_path}/analysis_data_episode_{episode}.pkl", 'wb') as f:
            pickle.dump(analysis_data, f)
            
        # Save training metrics as JSON for easy access
        metrics_json = {
            'rewards': self.training_metrics['rewards'],
            'losses': self.training_metrics['losses'],
            'epsilon_values': self.training_metrics['epsilon_values'],
            'episode_lengths': self.training_metrics['episode_lengths'],
            'final_ge_values': self.training_metrics['final_ge_values'],
            'model_selection_frequency': self.training_metrics['model_selection_frequency'].tolist()
        }
        
        with open(f"{save_path}/training_metrics_episode_{episode}.json", 'w') as f:
            json.dump(metrics_json, f, indent=2)
            
    def generate_visualizations(self, save_path: str, episode: int):
        """ç”Ÿæˆå®Œæ•´çš„å¯è§†åŒ–åˆ†æ"""
        
        # 1. Training Progress
        self._plot_training_progress(save_path, episode)
        
        # 2. Q-value Analysis
        self._plot_q_value_analysis(save_path, episode)
        
        # 3. Model Selection Frequency
        self._plot_model_selection_frequency(save_path, episode)
        
        # 4. Strategy Evolution
        self._plot_strategy_evolution(save_path, episode)
        
    def _plot_training_progress(self, save_path: str, episode: int):
        """ç»˜åˆ¶è®­ç»ƒè¿›åº¦"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # Rewards
        if self.training_metrics['rewards']:
            ax1.plot(self.training_metrics['rewards'], alpha=0.7)
            ax1.set_title('Episode Rewards')
            ax1.set_xlabel('Episode')
            ax1.set_ylabel('Total Reward')
            ax1.grid(True, alpha=0.3)
        
        # Losses
        if self.training_metrics['losses']:
            ax2.plot(self.training_metrics['losses'], alpha=0.7, color='red')
            ax2.set_title('Training Loss')
            ax2.set_xlabel('Training Step')
            ax2.set_ylabel('Loss')
            ax2.grid(True, alpha=0.3)
        
        # Epsilon values
        if self.training_metrics['epsilon_values']:
            ax3.plot(self.training_metrics['epsilon_values'], alpha=0.7, color='green')
            ax3.set_title('Exploration Rate (Epsilon)')
            ax3.set_xlabel('Step')
            ax3.set_ylabel('Epsilon')
            ax3.grid(True, alpha=0.3)
            
        # Episode lengths
        if self.training_metrics['episode_lengths']:
            ax4.plot(self.training_metrics['episode_lengths'], alpha=0.7, color='purple')
            ax4.set_title('Episode Lengths')
            ax4.set_xlabel('Episode')
            ax4.set_ylabel('Steps')
            ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{save_path}/training_progress_episode_{episode}.png", dpi=300, bbox_inches='tight')
        plt.close()
        
    def _plot_q_value_analysis(self, save_path: str, episode: int):
        """ç»˜åˆ¶Qå€¼åˆ†æ"""
        if not self.training_metrics['q_values']:
            return
            
        q_values_array = np.array(self.training_metrics['q_values'])
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Q-value distribution
        ax1.hist(q_values_array.flatten(), bins=50, alpha=0.7, edgecolor='black')
        ax1.set_title('Q-value Distribution')
        ax1.set_xlabel('Q-value')
        ax1.set_ylabel('Frequency')
        ax1.grid(True, alpha=0.3)
        
        # Q-value evolution over time
        if len(q_values_array) > 10:
            mean_q_values = np.mean(q_values_array, axis=1)
            ax2.plot(mean_q_values, alpha=0.8)
            ax2.set_title('Average Q-value Evolution')
            ax2.set_xlabel('Training Step')
            ax2.set_ylabel('Average Q-value')
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{save_path}/q_value_analysis_episode_{episode}.png", dpi=300, bbox_inches='tight')
        plt.close()
        
    def _plot_model_selection_frequency(self, save_path: str, episode: int):
        """ç»˜åˆ¶æ¨¡å‹é€‰æ‹©é¢‘ç‡"""
        plt.figure(figsize=(15, 6))
        
        model_indices = range(self.total_models)
        frequencies = self.training_metrics['model_selection_frequency']
        
        plt.bar(model_indices, frequencies, alpha=0.7)
        plt.title('Model Selection Frequency During Training')
        plt.xlabel('Model Index')
        plt.ylabel('Selection Count')
        plt.grid(True, alpha=0.3)
        
        # Highlight top models
        top_k = 10
        top_indices = np.argsort(frequencies)[-top_k:]
        for idx in top_indices:
            plt.bar(idx, frequencies[idx], color='red', alpha=0.8)
        
        plt.tight_layout()
        plt.savefig(f"{save_path}/model_selection_frequency_episode_{episode}.png", dpi=300, bbox_inches='tight')
        plt.close()
        
    def _plot_strategy_evolution(self, save_path: str, episode: int):
        """ç»˜åˆ¶ç­–ç•¥æ¼”åŒ–"""
        if len(self.training_metrics['final_ge_values']) < 2:
            return
            
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Final GE values over episodes
        ax1.plot(self.training_metrics['final_ge_values'], marker='o', alpha=0.7)
        ax1.set_title('Final GE Values Over Episodes')
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Final GE')
        ax1.grid(True, alpha=0.3)
        
        # Running average
        if len(self.training_metrics['final_ge_values']) > 10:
            window = min(10, len(self.training_metrics['final_ge_values']) // 4)
            running_avg = np.convolve(self.training_metrics['final_ge_values'], 
                                    np.ones(window)/window, mode='valid')
            ax1.plot(range(window-1, len(self.training_metrics['final_ge_values'])), 
                    running_avg, color='red', linewidth=2, label=f'Running Avg ({window})')
            ax1.legend()
        
        # Strategy convergence
        if self.training_metrics['rewards']:
            ax2.plot(np.cumsum(self.training_metrics['rewards']), alpha=0.7, color='green')
            ax2.set_title('Cumulative Rewards')
            ax2.set_xlabel('Episode')
            ax2.set_ylabel('Cumulative Reward')
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{save_path}/strategy_evolution_episode_{episode}.png", dpi=300, bbox_inches='tight')
        plt.close()
        
    def global_binary_strategy_selection(self, ensemble_predictions, all_ind_GE, 
                                       all_ind_NTGE=None, target_models=15, 
                                       perform_attacks_ensemble=None, 
                                       nb_traces_attacks=1000, plt_val=None, 
                                       correct_key=None, dataset="ASCAD", 
                                       nb_attacks=50, leakage="HW", byte=2,
                                       num_episodes=100, max_episode_length=None):
        """
        å…¨å±€äºŒè¿›åˆ¶ç­–ç•¥æ¨¡å‹é€‰æ‹©
        - ç›´æ¥å­¦ä¹ æœ€ä¼˜çš„äºŒè¿›åˆ¶å‘é‡è¡¨ç¤º
        - å…·å¤‡å…¨å±€é€‰æ‹©æ„è¯†
        - å®Œæ•´çš„å¯è§£é‡Šæ€§åˆ†æ
        
        Args:
            target_models: ç›®æ ‡æ¨¡å‹æ•°é‡
            num_episodes: è®­ç»ƒè½®æ•°
            max_episode_length: æœ€å¤§å•è½®é•¿åº¦ï¼ˆNoneä¸ºè‡ªåŠ¨è®¾ç½®ï¼‰
        """
        
        logger.info(f"ğŸ¯ å¼€å§‹å…¨å±€äºŒè¿›åˆ¶ç­–ç•¥DQNè®­ç»ƒ")
        logger.info(f"  æ€»æ¨¡å‹æ•°: {self.total_models}")
        logger.info(f"  ç›®æ ‡é€‰æ‹©: {target_models}")
        logger.info(f"  è®­ç»ƒè½®æ•°: {num_episodes}")
        
        if max_episode_length is None:
            max_episode_length = min(target_models * 3, 50)
        
        # Initialize performance tracking
        # ğŸ”§ ä¿®å¤: ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæœ€ä½³å•æ¨¡å‹ä½œä¸ºåŸºçº¿
        if all_ind_NTGE is not None:
            valid_ntge_indices = np.where(all_ind_NTGE != float('inf'))[0]
            if len(valid_ntge_indices) > 0:
                best_single_model = valid_ntge_indices[np.argmin(all_ind_NTGE[valid_ntge_indices])]
                best_ge = all_ind_GE[best_single_model]
                best_ntge = all_ind_NTGE[best_single_model]
            else:
                best_single_model = np.argmin(all_ind_GE)
                best_ge = all_ind_GE[best_single_model]
                best_ntge = float('inf')
        else:
            best_single_model = np.argmin(all_ind_GE)
            best_ge = all_ind_GE[best_single_model]
            best_ntge = float('inf')
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæ¨¡å‹ä½œä¸ºåŸºçº¿ç»“æœ
        best_combination = [best_single_model]
        best_ge_evolution = [best_ge]
        
        print(f"ğŸ¯ åˆå§‹åŒ–åŸºçº¿æ¨¡å‹:")
        print(f"   æ¨¡å‹ç´¢å¼•: {best_single_model}")
        print(f"   åŸºçº¿GE: {best_ge:.4f}")
        print(f"   åŸºçº¿NTGE: {best_ntge if best_ntge != float('inf') else 'âˆ'}")
        
        # Episode loop
        for episode in range(num_episodes):
            episode_start_time = time.time()
            
            print(f"\n{'='*60}")
            print(f"Episode {episode + 1}/{num_episodes}")
            print(f"{'='*60}")
            
            # Run one training episode
            episode_results = self._run_global_strategy_episode(
                ensemble_predictions, all_ind_GE, target_models,
                perform_attacks_ensemble, nb_traces_attacks, plt_val,
                correct_key, dataset, nb_attacks, leakage, byte,
                episode, max_episode_length, all_ind_NTGE, is_training=True
            )
            
            # Update best results using multi-objective evaluation
            current_best_score = self.evaluate_combination_quality(best_ge, best_ntge, len(best_combination))
            new_score = self.evaluate_combination_quality(episode_results['final_ge'], 
                                                        episode_results['final_ntge'], 
                                                        len(episode_results['selected_models']))
            
            if self.should_update_best_result(best_ge, best_ntge, 
                                              episode_results['final_ge'], 
                                              episode_results['final_ntge'], 
                                              current_best_score):
                best_ge = episode_results['final_ge']
                best_ntge = episode_results['final_ntge']
                best_combination = episode_results['selected_models'].copy()
                best_ge_evolution = episode_results['ge_evolution'].copy()
                best_ntge_evolution = episode_results['ntge_evolution'].copy()
                print(f"ğŸ‰ New best combination found!")
                print(f"   Models: {best_combination}")
                print(f"   Final GE: {best_ge:.4f}")
                print(f"   Final NTGE: {best_ntge if best_ntge != float('inf') else 'âˆ'}")
                print(f"   Model count: {len(best_combination)}")
                print(f"   Quality Score: {new_score:.2f} (prev: {current_best_score:.2f})")
            
            # Record episode metrics
            self.training_metrics['rewards'].append(episode_results['total_reward'])
            self.training_metrics['episode_lengths'].append(episode_results['episode_length'])
            self.training_metrics['final_ge_values'].append(episode_results['final_ge'])
            
            # Update model selection frequency
            for model_idx in episode_results['selected_models']:
                self.training_metrics['model_selection_frequency'][model_idx] += 1
            
            # Optimize model
            if len(self.memory) >= self.BATCH_SIZE:
                for _ in range(5):  # Multiple updates per episode
                    self.optimize_model()
            
            # Update target network periodically
            if episode % 10 == 0:
                self.update_target_network()
                
            episode_time = time.time() - episode_start_time
            
            print(f"Episode {episode + 1} completed in {episode_time:.2f}s")
            print(f"  Final GE: {episode_results['final_ge']:.4f}")
            print(f"  Final NTGE: {episode_results['final_ntge'] if episode_results['final_ntge'] != float('inf') else 'âˆ'}")
            print(f"  Total reward: {episode_results['total_reward']:.3f}")
            print(f"  Models selected: {len(episode_results['selected_models'])}")
            print(f"  Current best GE: {best_ge:.4f}")
            
            # Periodic analysis and visualization
            if (episode + 1) % 20 == 0:
                print(f"ğŸ“Š Generating intermediate analysis...")
                try:
                    analysis_dir = f"analysis_episode_{episode + 1}"
                    os.makedirs(analysis_dir, exist_ok=True)
                    self.save_analysis_data(analysis_dir, episode + 1)
                    self.generate_visualizations(analysis_dir, episode + 1)
                except Exception as e:
                    print(f"âš ï¸ Analysis generation failed: {str(e)}")
        
        print(f"\n{'='*60}")
        print(f"ğŸ è®­ç»ƒå®Œæˆ")
        print(f"{'='*60}")
        
        # Final evaluation with greedy policy
        print(f"ğŸ“Š æœ€ç»ˆè´ªå©ªç­–ç•¥è¯„ä¼°...")
        final_results = self._run_global_strategy_episode(
            ensemble_predictions, all_ind_GE, target_models,
            perform_attacks_ensemble, nb_traces_attacks, plt_val,
            correct_key, dataset, nb_attacks, leakage, byte,
            num_episodes, max_episode_length, all_ind_NTGE, is_training=False
        )
        
        # æ›´æ–°æœ€ä½³ç»“æœï¼ˆä½¿ç”¨å¤šç›®æ ‡è¯„ä¼°ï¼‰
        current_best_score = self.evaluate_combination_quality(best_ge, best_ntge, len(best_combination))
        final_score = self.evaluate_combination_quality(final_results['final_ge'], 
                                                      final_results['final_ntge'], 
                                                      len(final_results['selected_models']))
        
        if self.should_update_best_result(best_ge, best_ntge, 
                                          final_results['final_ge'], 
                                          final_results['final_ntge'], 
                                          current_best_score):
            best_ge = final_results['final_ge']
            best_ntge = final_results['final_ntge']
            best_combination = final_results['selected_models'].copy()
            best_ge_evolution = final_results['ge_evolution'].copy()
            best_ntge_evolution = final_results['ntge_evolution'].copy()
            print(f"ğŸ‰ Final greedy policy achieved new best!")
            print(f"   Quality Score: {final_score:.2f} (prev: {current_best_score:.2f})")
        
        # ğŸ”§ ç¡®ä¿è¿”å›æœ‰æ•ˆç»“æœï¼šå¦‚æœbest_combinationä¸ºç©ºï¼Œè¿”å›åŸºçº¿å•æ¨¡å‹
        if not best_combination:
            print(f"âš ï¸ è®­ç»ƒæœªæ‰¾åˆ°æ›´å¥½ç»„åˆï¼Œè¿”å›åŸºçº¿å•æ¨¡å‹")
            best_combination = [best_single_model]
            best_ge = all_ind_GE[best_single_model] 
            best_ntge = all_ind_NTGE[best_single_model] if all_ind_NTGE is not None else float('inf')
            best_ge_evolution = [best_ge]
        
        print(f"\nğŸ† æœ€ä½³ç»“æœ:")
        print(f"   é€‰æ‹©æ¨¡å‹: {best_combination}")
        print(f"   æ¨¡å‹æ•°é‡: {len(best_combination)}")
        print(f"   æœ€ç»ˆGE: {best_ge:.4f}")
        print(f"   æœ€ç»ˆNTGE: {best_ntge if best_ntge != float('inf') else 'âˆ'}")
        
        if best_ge == 0:
            print(f"   ğŸ¯ æˆåŠŸè¾¾åˆ° GE=0!")
            if best_ntge != float('inf'):
                print(f"   æœ€å°‘è½¨è¿¹æ•°: {best_ntge:.0f}")
            print(f"   æ”¶æ•›æ€§: å®Œç¾")
        elif best_ge < 10:
            print(f"   æ”¶æ•›æ€§: è‰¯å¥½")
        else:
            print(f"   æ”¶æ•›æ€§: éœ€æ”¹è¿›")
        
        # ç”ŸæˆQè¡¨å¯è§†åŒ–æŠ¥å‘Š
        if self.enable_q_visualization and self.q_visualizer is not None:
            try:
                print(f"\nğŸ“Š ç”ŸæˆQè¡¨å¯è§†åŒ–æŠ¥å‘Š...")
                self.q_visualizer.generate_comprehensive_report()
                print(f"âœ… Qè¡¨å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ Qè¡¨å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
        
        return best_combination, best_ge_evolution
        
    def _run_global_strategy_episode(self, ensemble_predictions, all_ind_GE, target_models,
                                   perform_attacks_ensemble, nb_traces_attacks, plt_val,
                                   correct_key, dataset, nb_attacks, leakage, byte,
                                   episode, max_steps, all_ind_NTGE=None, is_training=True):
        """
        è¿è¡Œå•ä¸ªå…¨å±€ç­–ç•¥å›åˆï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼ŒåŒ…å«NTGEè®¡ç®—ï¼‰
        """
        
        selected_models = []
        available_models = list(range(self.total_models))
        ge_evolution = []
        ntge_evolution = []
        
        # Initialize with baseline (best single model)
        if all_ind_NTGE is not None:
            # å¦‚æœæœ‰NTGEä¿¡æ¯ï¼Œä¼˜å…ˆé€‰æ‹©NTGEæœ€å°çš„æ¨¡å‹
            valid_ntge_indices = np.where(all_ind_NTGE != float('inf'))[0]
            if len(valid_ntge_indices) > 0:
                best_single_model = valid_ntge_indices[np.argmin(all_ind_NTGE[valid_ntge_indices])]
                current_ntge = all_ind_NTGE[best_single_model]
                print(f"ğŸ¯ é€‰æ‹©NTGEæœ€å°çš„å•æ¨¡å‹ {best_single_model}: NTGE={current_ntge:.0f}, GE={all_ind_GE[best_single_model]:.4f}")
            else:
                best_single_model = np.argmin(all_ind_GE)
                current_ntge = float('inf')
                print(f"âš ï¸ æ²¡æœ‰æ¨¡å‹èƒ½è¾¾åˆ°GE=0ï¼Œé€‰æ‹©GEæœ€å°çš„å•æ¨¡å‹ {best_single_model}: GE={all_ind_GE[best_single_model]:.4f}")
        else:
            best_single_model = np.argmin(all_ind_GE)
            current_ntge = float('inf')
            print(f"ğŸ“Š é€‰æ‹©GEæœ€å°çš„å•æ¨¡å‹ {best_single_model}: GE={all_ind_GE[best_single_model]:.4f}")
        
        current_ge = all_ind_GE[best_single_model]
        
        total_reward = 0.0
        step = 0
        
        # Episode loop
        while step < max_steps and len(selected_models) < target_models and available_models:
            step_start_time = time.time()
            
            # Get current state (binary representation)
            current_state = self.get_binary_state(selected_models)
            
            # Select action (which model to add)
            if available_models:
                chosen_model = self.select_action_epsilon_greedy(
                    current_state, step, available_models, 
                    selected_models, is_training
                )
                
                # Update state
                prev_state = current_state.clone()
                prev_ge = current_ge
                prev_ntge = current_ntge
                selected_models.append(chosen_model)
                available_models.remove(chosen_model)
                new_state = self.get_binary_state(selected_models)
                
                # Evaluate new combination
                temp_ensemble = ensemble_predictions[selected_models]
                
                try:
                    ensemble_GE, _ = perform_attacks_ensemble(
                        nb_traces_attacks, temp_ensemble, plt_val, correct_key,
                        dataset=dataset, nb_attacks=nb_attacks, 
                        shuffle=True, leakage=leakage, byte=byte
                    )
                    new_ge = ensemble_GE[-1]
                    # è®¡ç®—NTGE
                    new_ntge = NTGE_fn(ensemble_GE)
                    evaluation_successful = True
                except Exception as e:
                    print(f"âš ï¸ Step {step}: Attack evaluation failed: {str(e)}")
                    new_ge = current_ge * 1.1  # Penalty for failed evaluation
                    new_ntge = current_ntge  # ä¿æŒå½“å‰NTGE
                    evaluation_successful = False
                
                # Calculate reward with NTGE information
                reward = self.calculate_reward(prev_ge, new_ge, 
                                             selected_models[:-1], selected_models, 
                                             step, max_steps, prev_ntge, new_ntge)
                
                # Determine if episode should end
                done = (new_ge == 0.0 or 
                       len(selected_models) >= target_models or 
                       step >= max_steps - 1 or
                       not available_models)
                
                # Store transition in memory
                if is_training:
                    next_state = new_state if not done else None
                    self.memory.push(prev_state, chosen_model, next_state, reward, done)
                
                # Update for next iteration
                if evaluation_successful:
                    current_ge = new_ge
                    current_ntge = new_ntge
                    ge_evolution.append(new_ge)
                    ntge_evolution.append(new_ntge)
                else:
                    ge_evolution.append(current_ge)
                    ntge_evolution.append(current_ntge)
                
                total_reward += reward
                
                step_time = time.time() - step_start_time
                
                if is_training or step % 5 == 0:  # Reduce output in evaluation mode
                    ntge_str = f"{new_ntge:.0f}" if new_ntge != float('inf') else "âˆ"
                    print(f"  Step {step + 1}: Model {chosen_model} â†’ GE {new_ge:.4f}, NTGE {ntge_str} "
                          f"(Reward: {reward:.3f}, Time: {step_time:.2f}s)")
                
                # Early stopping if perfect
                if new_ge == 0.0:
                    print(f"ğŸ¯ Perfect attack achieved at step {step + 1}!")
                    if new_ntge != float('inf'):
                        print(f"   NTGE: {new_ntge:.0f} è½¨è¿¹")
                    break
                    
            step += 1
        
        final_ge = ge_evolution[-1] if ge_evolution else current_ge
        final_ntge = ntge_evolution[-1] if ntge_evolution else current_ntge
        
        return {
            'selected_models': selected_models,
            'final_ge': final_ge,
            'final_ntge': final_ntge,
            'ge_evolution': ge_evolution,
            'ntge_evolution': ntge_evolution,
            'total_reward': total_reward,
            'episode_length': step,
            'converged': final_ge < 10.0
        }

    def find_minimum_traces_for_ge_zero(self, selected_models, ensemble_predictions,
                                       plt_attack, correct_key, dataset, leakage, 
                                       byte, perform_attacks_ensemble, max_traces=20000):
        """
        æ‰¾åˆ°è¾¾åˆ° GE=0 çš„æœ€å°‘æ”»å‡»è½¨è¿¹æ•°
        ä½¿ç”¨äºŒåˆ†æœç´¢ä¼˜åŒ–æœç´¢è¿‡ç¨‹
        """
        temp_ensemble = ensemble_predictions[selected_models]
        
        # é¦–å…ˆéªŒè¯æ˜¯å¦èƒ½åœ¨æœ€å¤§è½¨è¿¹æ•°å†…è¾¾åˆ° GE=0
        logger.info(f"éªŒè¯æ˜¯å¦èƒ½åœ¨ {max_traces} è½¨è¿¹å†…è¾¾åˆ° GE=0...")
        ensemble_GE, _ = perform_attacks_ensemble(
            max_traces, temp_ensemble, plt_attack, correct_key,
            dataset=dataset, nb_attacks=100, shuffle=True, 
            leakage=leakage, byte=byte
        )
        
        if ensemble_GE[-1] != 0:
            logger.warning(f"æ— æ³•åœ¨ {max_traces} è½¨è¿¹å†…è¾¾åˆ° GE=0ï¼Œæœ€ç»ˆ GE={ensemble_GE[-1]}")
            return None, None, ensemble_GE
        
        logger.info(f"âœ… ç¡®è®¤å¯ä»¥è¾¾åˆ° GE=0ï¼Œå¼€å§‹äºŒåˆ†æœç´¢æœ€å°‘è½¨è¿¹æ•°...")
        
        # äºŒåˆ†æœç´¢æ‰¾åˆ°æœ€å°‘è½¨è¿¹æ•°
        left, right = 1, max_traces
        best_traces = max_traces
        best_ge_curve = ensemble_GE
        
        while left <= right:
            mid = (left + right) // 2
            
            logger.info(f"æµ‹è¯• {mid} è½¨è¿¹...")
            ensemble_GE, _ = perform_attacks_ensemble(
                mid, temp_ensemble, plt_attack, correct_key,
                dataset=dataset, nb_attacks=50, shuffle=True,
                leakage=leakage, byte=byte
            )
            
            if ensemble_GE[-1] == 0:
                # æˆåŠŸè¾¾åˆ° GE=0ï¼Œå°è¯•æ›´å°‘çš„è½¨è¿¹
                best_traces = mid
                best_ge_curve = ensemble_GE
                right = mid - 1
                logger.info(f"  âœ“ GE=0 è¾¾æˆï¼Œç»§ç»­æœç´¢æ›´å°‘è½¨è¿¹...")
            else:
                # æœªè¾¾åˆ° GE=0ï¼Œéœ€è¦æ›´å¤šè½¨è¿¹
                left = mid + 1
                logger.info(f"  âœ— GE={ensemble_GE[-1]:.2f}ï¼Œéœ€è¦æ›´å¤šè½¨è¿¹...")
        
        logger.info(f"ğŸ¯ æœ€å°‘æ”»å‡»è½¨è¿¹æ•°: {best_traces}")
        
        # è®¡ç®— NTGE (Normalized Traces to Guessing Entropy)
        ntge = NTGE_fn(best_ge_curve)
        
        return best_traces, ntge, best_ge_curve
    
    def benchmark_and_select_optimal_traces(self, selected_models, ensemble_predictions,
                                          plt_attack, correct_key, dataset, leakage, 
                                          byte, perform_attacks_ensemble, max_traces=20000):
        """
        å¯¹é€‰ä¸­çš„æ¨¡å‹ç»„åˆè¿›è¡Œæœ€ä¼˜è½¨è¿¹æ•°æœç´¢ï¼ˆä»¥NTGEä¸ºæ ¸å¿ƒæŒ‡æ ‡ï¼‰
        """
        if not selected_models:
            return None, None, None
            
        print(f"ğŸ” æœç´¢æœ€ä¼˜è½¨è¿¹æ•°ï¼ˆåŸºäºNTGEï¼‰...")
        print(f"  é€‰ä¸­æ¨¡å‹: {selected_models}")
        print(f"  æ¨¡å‹æ•°é‡: {len(selected_models)}")
        
        temp_ensemble = ensemble_predictions[selected_models]
        trace_candidates = [500, 1000, 2000, 3000, 5000, 8000, 10000, 15000, 20000]
        trace_candidates = [t for t in trace_candidates if t <= max_traces]
        
        optimal_traces = None
        optimal_ntge = float('inf')
        optimal_ge_curve = None
        best_ntge = float('inf')
        
        for nb_traces in trace_candidates:
            print(f"  æµ‹è¯• {nb_traces} è½¨è¿¹...")
            
            try:
                ensemble_GE, _ = perform_attacks_ensemble(
                    nb_traces, temp_ensemble, plt_attack, correct_key,
                    dataset=dataset, nb_attacks=30, shuffle=True, 
                    leakage=leakage, byte=byte
                )
                
                final_ge = ensemble_GE[-1]
                # ä½¿ç”¨æ­£ç¡®çš„NTGEå‡½æ•°è®¡ç®—
                ntge = NTGE_fn(ensemble_GE)
                
                print(f"    â†’ GE: {final_ge:.3f}, NTGE: {ntge if ntge != float('inf') else 'âˆ'}")
                
                # ä¼˜å…ˆé€‰æ‹©NTGEæœ€å°çš„ç»„åˆ
                if ntge < best_ntge:
                    best_ntge = ntge
                    optimal_traces = nb_traces
                    optimal_ntge = ntge
                    optimal_ge_curve = ensemble_GE
                    print(f"    ğŸ¯ æ–°çš„æœ€ä¼˜è§£: {nb_traces} è½¨è¿¹, NTGE: {ntge}")
                
                # å¦‚æœè¾¾åˆ°GE=0ï¼Œå¯ä»¥æå‰åœæ­¢ï¼ˆä½†ç»§ç»­æ£€æŸ¥æ˜¯å¦æœ‰æ›´å°çš„NTGEï¼‰
                if final_ge == 0 and ntge != float('inf'):
                    print(f"    âœ… è¾¾åˆ°GE=0ï¼ŒNTGE: {ntge}")
                    
            except Exception as e:
                print(f"    â†’ æµ‹è¯•å¤±è´¥: {str(e)}")
                continue
        
        if optimal_traces is None:
            print(f"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆ")
        else:
            print(f"ğŸ† æœ€ä¼˜è§£: {optimal_traces} è½¨è¿¹, NTGE: {optimal_ntge}")
        
        return optimal_traces, optimal_ntge, optimal_ge_curve

# Set up logger
logger = logging.getLogger(__name__) 